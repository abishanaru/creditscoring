---
title: "Credit Scoring SBD2"
author: "Abishan Arumugavel, Vladyslav Gorbunov, Gilles Nikles, Josua Reich"
date: "2023-10-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


In the rapidly evolving landscape of financial technology, credit scoring remains a cornerstone in determining the creditworthiness of applicants. This paper delves into the realm of predictive modeling, using a dataset provided by our professors, enriched with comprehensive credit scoring information. The overarching objective of our study is to train a model that efficiently and accurately determines the eligibility of individuals for credit approval.

The task set forth by our professors involves a comprehensive journey through four key exercises. The first exercise is centered around a thorough analysis of the dataset. Here, we delve into understanding the underlying structures, identifying critical variables, and most importantly, balancing the dataset to ensure a fair representation of diverse credit scenarios.

Subsequently, in the second exercise, we focus on training and testing a logistic classifier. This step is crucial in establishing a baseline model from which further improvements can be measured.

The third exercise involves enhancing the predictive performance of the model. This stage is particularly challenging and significant, as it entails fine-tuning the model to ensure it captures the nuances of credit scoring with greater accuracy and efficiency.

Finally, the fourth exercise invites us to step into the practical world. Here, we explore the various challenges a company may face if they were to implement our model in a real-world scenario. This exercise not only grounds our theoretical work in reality but also provides valuable insights into the practical implications and considerations in deploying machine learning models in the credit industry.

This paper aims to not only present a robust model for credit scoring but also to contribute to the broader understanding of how machine learning can be effectively utilized in financial decision-making processes.


## Load packages and data
```{r}
libraries = c("readr", "tidyverse", "ggplot2", "dplyr", "reshape2", "plotly", "caret", "DescTools", "pROC", "ROCR", "ROSE", "Boruta", "RColorBrewer", "ggcorrplot", "PerformanceAnalytics", "corrr", "networkD3", "reshape", "knitr", "dlookr")

lapply(libraries, function(x) if (!(x %in% installed.packages())) {
  install.packages(x)
})

lapply(libraries, library, quietly = TRUE, character.only = TRUE)

rm(list=ls())

loan_sample <- read_csv("loan_sample_8.csv")

# Create a copy of the original dataset
loan_sample_original <- loan_sample
```


## Excercise 1

### 1.1 Describe the data. Specifically:

#### 1.1.1: Check and report the structure of the data set.
```{r show_col_types = False}
str(loan_sample)
```

```{r}
# Checking for NAs
any(is.na(loan_sample))
```
The result of "FALSE" from the check indicates that there are no missing values (NAs) in the "loan_sample" dataset.


#### 1.1.2: How many numeric and how many categorical variables are included in the data? What categorical variable has the most levels in it?
```{r show_col_types = False}
# First we have to check whether all variables have been imported in the correct format. 
first_overview <- overview(loan_sample)
plot(first_overview)
```
It is clearly visible that the columns with the data type character still have to be converted to factors. In addition, the "Status" column is currently still a numeric data type. This must also be converted to factor, as it is a binary variable. 

```{r}
loan_sample <- loan_sample %>%
  mutate_if(is.character, as.factor) %>% # Convert all character columns to factors
  mutate(Status = as.factor(Status)) # Convert 'Status' column to factor

# Plot to see, if the datatypes are correct
second_overview <- overview(loan_sample)
plot(second_overview)
```
That looks right, now we can count the number of variables.

```{r}
# Count numeric variables in 'loan_sample'
numeric_vars_count <- sum(sapply(loan_sample, is.numeric))

# Count categorical (factor) variables in 'loan_sample'
categorical_vars_count <- sum(sapply(loan_sample, is.factor))

# Print counts of numeric and categorical variables
cat("There are", numeric_vars_count, "numeric variables and", categorical_vars_count, "categorical variables in the dataset.")

```

```{r show_col_types = False}
categorical_levels <- sapply(loan_sample, function(x) if(is.factor(x)) length(unique(x)) else NA)

# Find the name of the categorical variable with the most levels
cat_var_most_levels <- names(which.max(categorical_levels))

# Find the number of levels for this variable
levels_count <- max(categorical_levels, na.rm = TRUE)

# Print the variable name and the number of levels
cat("The categorical variable with the most levels in the dataset is:", cat_var_most_levels, "with", levels_count, "levels.")
```



#### 1.1.3: Summarize the variables. Discuss the summary statistics obtained.
```{r show_col_types = False}
# Categorial Label with the most Levels
summary(loan_sample)
```
Our loan data includes information on 40,000 loans. The average loan size is about $11,687, but this number can be as low as $1,000 or as high as $40,000, showing that loan amounts are different for different people. The average interest rate on a loan is 12.62%, but some people get lower rates like 5.31%, and others get higher rates up to 27.49%. This shows there are many different interest rates that people are getting. People who have taken out loans make an average of $63,400 a year, but there is a big difference in how much money people make. Some make more, and some make less. The debt-to-income ratio (DTI) tells us how much debt people have compared to their income. On average, this number is 18.22% for the loans we looked at. Looking at how many credit lines people have open, the average is 10, but in total, they might have had up to 21 over time. This tells us about how many loans or credit cards people might be using or have used before. The average amount people owe on their credit lines is $11,995, and they use about half of the credit available to them. This tells us about how much of their available loan money they are using. People also pay about $1,818 in interest on average. This is extra money they pay in addition to the loan they took out. When we look at all the loans and credit people have, the total amount they owe on average is $99,100. This shows that people have a lot of loans or owe a lot of money on their credit lines. Finally, the average limit people have on their credit lines is $24,194. This is how much the bank or loan company lets them borrow in total.

#### 1.1.4: Check the levels of the target variable by choosing the appropriate visualization. Is the target variable balanced?
```{r show_col_types = False}
ggplot(loan_sample, aes(x = Status, fill = factor(Status))) + 
  geom_bar() +
  scale_fill_brewer(palette = "Set1") + 
  labs(title = 'Distribution of Target Variable (Status)', x = 'Status', y = 'Count') +
  theme_minimal()

```


1.1.5: Check the distribution of the numeric variables in the data set (include different visual representations).
```{r show_col_types = False}
# Histograms
loan_sample %>% 
  select_if(is.numeric) %>% 
  gather(key = "variable", value = "value") %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30) +
  facet_wrap(~variable, scales = 'free_x') +
  theme_minimal()
```
```{r show_col_types = False}
# Boxplots
loan_sample %>% 
  select_if(is.numeric) %>% 
  gather(key = "variable", value = "value") %>%
  ggplot(aes(y = value)) +
  geom_boxplot() +
  facet_wrap(~variable, scales = 'free') +
  theme_minimal()
```


### 1.2: Investigating outliers with a box plot
Elaborate your view on how to proceed in dealing with the outliers and – if necessary – take appropriate action.
```{r}
# Create a boxplot for all numeric variables
boxplot(scale(loan_sample))
```

### 1.3: Investigating the distribution of the numeric features per the two levels of our target feature
(i.e. default vs non-default). Discuss the visualizations. Which variables seem to be relevant in predicting the target feature?
```{r}
# Plotting the density of numeric features by the levels of the target feature
loan_sample %>%
  select(where(is.numeric)) %>%  # Select only numeric columns
  pivot_longer(cols = !c(Status), names_to = "variable", values_to = "value") %>%  # Reshape the data
  ggplot(aes(x = value, fill = Status)) +  # Map 'Status' to 'fill' aesthetic
  geom_density(alpha = 0.5) +  # Use density plot
  facet_wrap(~variable, scales = 'free_x') +  # Facet by variable
  theme_minimal() +
  labs(title = "Density Plots of Numeric Variables by Status", x = "Value", y = "Density") +
  scale_fill_brewer(palette = "Set1")  # Use a color palette for better distinction

```
Variables to pay attention to would be those with:

- Distinct peaks at different values for each group.
- Varying spreads (some might be more tightly clustered for one group and more spread out for the other).
- Different skewness (one group could be skewed left or right compared to the other).

Loan Amount (loan_amnt): If defaults show consistently higher or lower loan amounts than non-defaults, this variable might be predictive. The relevance would be stronger if there is a noticeable difference in the interquartile range between the two statuses.

Interest Rate (int_rate): Higher interest rates may be associated with higher default risk, so if defaults have higher median interest rates than non-defaults, this variable could be important.

Annual Income (annual_inc): While high-income individuals can also default, a general pattern where lower-income applicants have a higher proportion of defaults could signal relevance.

Debt-to-Income Ratio (dti): A higher DTI often indicates financial strain, which could increase default risk. If the DTI is higher for defaults, it would be a significant predictor.

Open Credit Lines (open_acc): This variable, by itself, may not be as predictive unless it shows a consistent pattern with default status. However, it could become relevant when combined with other variables like DTI.

Revolving Balance (revol_bal): High revolving balances could indicate potential financial stress or aggressive credit use, which might correlate with default risk.

Revolving Utilization Rate (revol_util): Higher utilization rates are often seen as a red flag for credit risk, making this variable potentially predictive.

Total Number of Credit Lines (total_acc): Like open credit lines, the total number of credit lines might provide insight when analyzed in conjunction with other variables.

Total Received Interest (total_rec_int): If defaults tend to accumulate more interest, this suggests they may have longer-standing debts or higher rates, which could be predictive.

Total Current Balance (tot_cur_bal) and Total Revolving High Credit/Credit Limit (total_rev_hi_lim): These variables might indicate the overall credit engagement and financial leverage of a borrower. Large balances and high credit limits, if associated with defaults, could suggest that borrowers with higher credit engagements are at a higher risk of default.

### 1.4: Use a bar plot visualization to investigate the associations between the categorical variables and the target feature.
```{r}
# Generate a bar plot for each categorical variable
categorical_vars <- c("home_ownership", "verification_status", "purpose", "addr_state")
# Loop over categorical variables and plot using aes() and without aes_string()
for (cat_var in categorical_vars) {
  # Check if the column exists to avoid errors
  if (!cat_var %in% names(loan_sample)) {
    message(paste("Skipping", cat_var, "as it is not found in the dataset."))
    next
  }
  
  # Create the plot
  plot <- loan_sample %>%
    group_by(.data[[cat_var]], Status) %>%
    summarise(Count = n(), .groups = 'drop') %>%
    ggplot(aes(x = .data[[cat_var]], y = Count, fill = as.factor(Status))) +
    geom_bar(stat = "identity", position = position_dodge()) +
    labs(title = paste("Bar plot of", cat_var, "by Status"), x = cat_var, y = "Count") +
    scale_fill_discrete(name = "Status") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) # Rotate x labels for readability
  
  print(plot) # Print the plot
}

```

### 1.5: Visualize the correlations that emerge between the numerical features. Discuss the results. Which variables are highly correlated? Decide whether you keep all variables.
```{r}
# Calculate the correlation matrix of numeric features
numeric_vars <- loan_sample %>% select_if(is.numeric)
cor_matrix <- cor(numeric_vars, use = "complete.obs")  # use="complete.obs" handles missing values by case-wise deletion

# Melt the correlation matrix into long format

melted_cor_matrix <- melt(cor_matrix)
# Create a heatmap of the correlation matrix
ggplot(data = melted_cor_matrix, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1),
        axis.text.y = element_text(size = 12)) +
  coord_fixed()

```

Strong Correlations: Variables with high absolute values of correlation (close to 1 or -1) are considered strongly correlated. These could indicate a redundant or derived relationship where one variable can be predicted from the other.

Moderate to Weak Correlations: Values that are closer to 0 indicate a weaker relationship.

### 1.6: Plot an interactive scatter plot of the association between the loan amount requested and the annual income of the borrower. Discuss the plot. What can you tell about the association?
```{r}
# Create a ggplot object with loan amount and annual income
p <- ggplot(loan_sample, aes(x = annual_inc, y = loan_amnt)) +
  geom_point(alpha = 0.5) + 
  labs(title = "Scatter plot of Loan Amount vs Annual Income",
       x = "Annual Income",
       y = "Loan Amount") +
  theme_minimal()

# Convert ggplot object to a plotly object
p_interactive <- ggplotly(p)
# p_interactive

```

The areas of high density indicate common combinations of income and loan amounts, which could suggest standard loan products or typical borrower profiles. A wide range of incomes with a relatively narrow range of loan amounts might suggest that the loan amount is less sensitive to income past a certain threshold.

### 1.7: Create a new balanced data set where the two levels of the target variable will be equally represented; Create a bar plot of the newly created target variable. Why is this step necessary?
```{r}
loan_sample_balanced <- ovun.sample(Status ~ ., data=loan_sample, method = "under")
loan_sample_under <- data.frame(loan_sample_balanced[["data"]])

# Plot the balance
ggplot(loan_sample_under, aes(x = Status, fill = Status, group = Status)) +
  geom_bar() +
  ylab("Count") +
  xlab("Status of the loan") +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal() +
  ggtitle("New balanced dataset")
```
Balancing a dataset is important for training classification models because it prevents the model from being biased towards the majority class and potentially ignoring the minority class. This is particularly true for datasets where the outcome class of interest (e.g., default on a loan) is much less common than the non-interest class (e.g., non-default). Balancing helps to improve the generalization of the model and its performance on unseen data, especially for the minority class.


## Excercise 2
• Train and test a logistic classifier. Specifically:
– Divide the sample into training and testing set using 70% for training the algorithm. 
– Train the classifier and report the coefficients obtained and interpret the results. 
– Plot the ROC and the Precision/Recall Curve and interpret the results.
– Produce the confusion matrix and interpret the results.
– Report the AUC values and the overall accuracy and interpret the results.

```{r}
# Set seed for reproducibility
set.seed(7)

# Split the data into training (70%) and testing (30%) sets
splitIndex <- createDataPartition(loan_sample_under$Status, p = 0.7, list = FALSE)
training_set <- loan_sample_under[splitIndex,]
testing_set <- loan_sample_under[-splitIndex,]
```

### Brotuto algo
In the next step, we run the Boruta algo.
```{r, echo=FALSE}
loan_sample_under$Status <- as.factor(loan_sample_under$Status)
boruta_output <- Boruta(Status~., data = loan_sample_under, doTrace=2)
```


Next, we extract and print the significant attributes. 
```{r}
boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
print(boruta_signif)
```


Next, we visualize the results. 
```{r}
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance") 
```

Finally, we can check for multicollinearity which can also be part of the feature selection process. Remember - multicollinearity - one predictor variable in a multiple regression model can be linearly predicted from the others with a high degree of accuracy. This can lead to skewed or misleading estimates and conclusions

In the next step, we look at the correlations that emerge between the variables. Which correlation are you worried about? 
```{r}
# Convert categorical variables to factors


correlations = cor(loan_sample_under[-c(3,4,6,7,14)])
corrplot(correlations) 
```

We can also test for the significance of the correlation. The cor_pmat function allow us to visualize the correlations explicitly marking the insignificant correlations. 

```{r}
p_value_mat <- cor_pmat(data_new_under[,-c(20:24)])
ggcorrplot(correlations, type = "lower", p.mat = p_value_mat) 
```

Looking at the correlation plots and the output from the Boruta algo, we would keep all variables apart from ratio17. 

```{r}
data_new_under <- data_new_under[-10]
```









## alter Code ABI / Vla
```{r}

# Split the data into training(70%) and testing(30%) data sets.
splitIndex <- createDataPartition(y = loan_data$Status, p = 0.7, list = FALSE)

# Training data (70%)
train_data <- loan_data[splitIndex, ]
# Checking if the data distribution in the train_data
PercTable(train_data$Status)

# Testing data (30%)
test_data <- loan_data[-splitIndex, ]
# Checking if the data distribution in the test_data
PercTable(test_data$Status)

# Train a logistic regression model (all variables as input)
model <- glm(Status ~ ., data = train_data, family = "binomial")

# Display the coefficients
summary(model)

# Predict probabilities on the test set
predictions_prob <- predict(model, type = "response", newdata = test_data)

# ROC curve using pROC
roc_obj <- roc(test_data$Status, predictions_prob)
plot(roc_obj, main="ROC Curve")
auc(roc_obj)

# Precision-Recall Curve using ROCR
pred_obj <- prediction(predictions_prob, test_data$Status)
pr <- performance(pred_obj, measure="prec", x.measure="rec")
plot(pr, main="Precision-Recall Curve")

# Confusion matrix
predictions_class <- ifelse(predictions_prob > 0.5, 1, 0)
cm <- confusionMatrix(as.factor(predictions_class), as.factor(test_data$Status))
print(cm)

# Overall accuracy
accuracy <- sum(diag(cm$table)) / sum(cm$table)
print(paste("Overall Accuracy: ", accuracy))
```
# Excercise 3
Habe bereits versucht mit factors die accuracy des Models zu verbessern. Hat jedoch keinen Einfluss.
Mann könnte einen kurzen Text darüber schreiben und erklären, dass wir es versucht haben.


